<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IconQA</title>
    <link rel="icon" href="res/knowledge-icon.png" type="image/icon type">
    <link rel="stylesheet" href="main.css">
    <link rel="stylesheet" href="nav.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="index.js" defer></script>
</head>
<body>
    <div id="nav">
        <div id="icon">
            <img src="res/knowledge-icon.png" id="nav-icon">
            <p>IconQA</p>
        </div>
        <div>
            <a class="nav-button" href="#home">Home</a>
            <a class="nav-button" href="#about">About</a>
            <a class="nav-button" href="#overview">Overview</a>
            <a class="nav-button" href="#icon645">Icon645</a>
            <a class="nav-button" href="#download">Download</a>
            <a class="nav-button" href="#citation">Citation</a>
            <a class="nav-button" href="explore.html">Explore</a>
            <a class="nav-button" href="visualization.html">Visualizations</a>
        </div>
    </div>

    <!-- anchor for the home button -->
    <div id="home" style="position: absolute; top: 0;"></div>

    <!-- banner -->
    <div id="title">
        <div id="title-wrapper">
            <img src="res/knowledge-icon.png" id="title-icon">
            <p id="title-text">IconQA</p>
        </div>
        <p id="subtitle-text">
            A New Benchmark for <br> 
            Abstract Diagram Understanding and <br> 
            Visual Language Reasoning
        </p>
        <!-- raise the title and subtitle up a little bit -->
        <p id="title-padding-bottom">&nbsp;</p>
    </div>


    <!-- the main body of the page -->
    <!-- each section uses an empty div as an anchor -->
    <div id="body">
        <div class="section">
            <div id="about" class="anchor"></div>
            <h1>What is IconQA?</h1>
            <div id="example-box">
                <img class="example-img" src="res/example1.png" alt="example1.png"/>
                <img class="example-img" src="res/example2.png" alt="example2.png"/>
                <img class="example-img" src="res/example3.png" alt="example3.png"/>
            </div>
            <p>Current visual question answering (VQA) tasks mainly consider answering 
                human-annotated questions for natural images in the daily-life context. 
                In this work, we propose a new challenging benchmark, icon question 
                answering (IconQA), which aims to highlight the importance of 
                <strong> abstract diagram understanding </strong> and 
                <strong>comprehensive cognitive reasoning </strong> 
                in real-world diagram word problems. 
                For this benchmark, we build up a large-scale 
                IconQA dataset that consists of three sub-tasks: multi-image-choice, 
                multi-text-choice, and filling-in-the-blank. Compared to existing VQA 
                benchmarks, IconQA requires not only <strong>perception skills</strong> like object 
                recognition and text understanding, but also diverse <strong>cognitive reasoning</strong> 
                skills, such as geometric reasoning, commonsense reasoning, and 
                arithmetic reasoning.
            </p>

        </div>
        <hr>
        <div class="section">
            <div id="overview" class="anchor"></div>
            <h1>Overview</h1>
            <div class="makecol">
                <div id="overview-left">
                    <p>IconQA provides diverse visual question answering questions that require:</p>
                    <ul>
                        <li>basic common sense knowledge</li>
                        <li>comprehensive visual reasoning skills </li>
                        <li>abstract diagram recognition </li>
                    </ul>
                    <p>There are three different sub-tasks in IconQA:</p>
                    <ul>
                        <li>57,672 image choice MC questions</li>
                        <li>31,578 text choice MC questions</li>
                        <li>18,189 fill-in-the-blank questions</li>
                    </ul>
                    <p>
                        The IconQA dataset can be accessed <a href="https://github.com/lupantech/IconQA" target="blank" class="ext-link">here</a>.
                    </p>
                    <table>
                        <thead>
                            <tr>
                                <th>Tasks</th><th>Train</th><th>Validation</th><th>Test</th><th>Total</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Multi-image-choice</td><td>34,603</td><td>11,535</td><td>11,535</td><td>57,672</td>
                            </tr>
                            <tr>
                                <td>Multi-text-choice</td><td>18,946</td><td>6,316</td><td>6,316</td><td>31,578</td>
                            </tr>
                            <tr>
                                <td>Multi-image-choice</td><td>10,913</td><td>3,638</td><td>3,638</td><td>18,189</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <div id="overview-right">
                    <canvas id="task-pie-chart"></canvas>
                </div>
            </div>
        </div>
        <hr>
        <div class="section">
            <div id="icon645" class="anchor"></div>
            <h1>Icon645</h1>
            <div class="col-wrapper">
                <div class="col">
                    <img src="res/icon645.png">
                </div>
                <div class="col">
                    <p>
                        In adition to IconQA, we also present Icon645,
                        a dataset of icons for training abstract image 
                        classifiers
                    </p>
                    <ul>
                        <li><strong>645,687</strong> colored icons</li>
                        <li><strong>377</strong> different icon classes</li>
                    </ul>
                    <p>
                        Check out the dataset 
                        <a href="https://github.com/lupantech/IconQA" target="blank" class="ext-link">here</a>.
                    </p>
                </div>
            </div>
        </div>
        <hr>
        <div class="section">
            <div id="download" class="anchor"></div>
            <h1>Download</h1>
            <p>
                Our dataset is distributed under the CC BY-NC-SA (Attribution-NonCommercial-ShareAlike) license, 
                which allows anyone to use our dataset for free under the following terms:
            </p>
            <ul>
                <li>You must give appropriate credit, provide a link to the license, and indicate if changes were made.</li>
                <li>You must not use the material for commercial purposes.</li>
                <li>If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li>
            </ul>
            <p>
                If you agree with the terms listed above, you can access the IconQA and the Icon645 dataset <a href="https://github.com/lupantech/IconQA" target="blank" class="ext-link">here</a>.
            </p>
            <img src="res/ccbyncsa.png" id="cc-img">

        </div>
        <hr>
        <div class="section">
            <div id="citation" class="anchor"></div>
            <h1>Citation</h1>
            If the paper or the dataset inspires you, please cite us:
            <p class="bibtax">
                @inproceedings{lu2021iconqa,
                <br>&emsp; title = {IconQA: A New Dataset for Abstract Diagram Understanding and Visual Language Reasoning},
                <br>&emsp; author = {Lu, Pan and Qiu, Liang and Chen, Jiaqi and Xia, Tony and Zhao, Yizhou and Zhang, Wei and Yu, Zhou and Liang, Xiaodan and Zhu, Song-Chun},
                <br>&emsp; booktitle = {Submitted to the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks},
                <br>&emsp; year = {2021}
                <br>}
            </p>
        </div>
    </div>

    <!-- footer -->
    <div id="footer">
        Â© 2021 Center for Vision, Cognition, Learning, and Autonomy at UCLA
    </div>
</body>
</html>